{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xTKiGPkJ5qRy"
   },
   "source": [
    "# PSET 2: Matrix Inverse\n",
    "\n",
    "...\n",
    "\n",
    "Try not to add additional cells nor rearrange any that exist now as it will mess with our autograder (but feel free to open another Colab/notebook on the side).\n",
    "\n",
    "### Make sure to submit your final notebook with all of your solutions to Gradescope!\n",
    "[Direct Gradescope Link](https://www.gradescope.com/courses/820552)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TOusEnXk5qRz"
   },
   "source": [
    "### Before Starting\n",
    "\n",
    "This problem set uses NVCC (NVIDIA CUDA Compiler) within Google Colab to compile and run CUDA code. Google Colab provides a convenient (and free!) environment with GPU support for executing these programs.  At the top right corner, under the arrow dropdown next to the `Connect` button, select \"Change runtime type\" and choose `T4 GPU` for this problem set.  Note, across your account, you can only have one runtime connected to a GPU at a given time.\n",
    "\n",
    "We're in an interactive _Python_ notebook.  So, of course, running C++ code is not going to be as easy as it is to run Python code.  That said, we'll once again use helpers to make this cleaner.\n",
    "\n",
    "Our plugin `%%gpurun` saves, compiles, and runs your CUDA code in the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_t8fU8-85qR0",
    "outputId": "0179e509-7fc2-4896-fbdd-21ea0265cf78"
   },
   "outputs": [],
   "source": [
    "# make sure CUDA is installed\n",
    "!nvcc --version\n",
    "\n",
    "# make sure you have a GPU runtime (if this fails go to runtime -> change runtime type)\n",
    "!nvidia-smi\n",
    "\n",
    "# Install some magic to run and save .cu C++ CUDA programs\n",
    "!curl -o ./gpu_runner.py https://raw.githubusercontent.com/COMS-BC3159-F24/helpers/main/gpu_runner.py\n",
    "%load_ext gpu_runner\n",
    "\n",
    "# to learn about how to do more fancy things with CUDA using this API see:\n",
    "# https://nvcc4jupyter.readthedocs.io/en/latest/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3KsZoHydPptM"
   },
   "source": [
    "## Problem 1: Inverse!\n",
    "\n",
    "In the previous problem sets, you've explored how we can compute matrix multiplication via various methods and platforms (na√Øvely in Python, using libraries, C++, memory-aware methods, and on a GPU).  That is one piece necessary for programatically solving a linear system. *Now*, let's write a kernel to compute a **matrix inverse** which we can then combine with our prior matrix multiplication to solve a linear system.\n",
    "\n",
    "\n",
    "As always, we have provided some starter code for you, so your job is to fill in the missing pieces.  We've structured the code for you, so let's describe the pieces:\n",
    "\n",
    "\n",
    "### `matrix_inverse_inner` (5 points) \n",
    "This device function will do all of the heavy lifting and actually implement the matrix inverse function. It takes in the input matrix and outputs the inverse. You should assume these pointers are already in shared memory. There is also an additional input of additional shared temporary memory which you may or may not need to use (and you will get to specify how big it is in later functions). You can implement this in any way, but the simplest solution is to use the Gauss-Jordan elimination method. You can see some graphical examples of \"elementary row operations\" [at this link](https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html). \n",
    "\n",
    "\n",
    "You can simply walk through the rows in order (do not swap any rows) and for each row divide it to ensure that it is leading with a 1 and then subtract down along the rest of the rows to ensure they are all leading with a 0. If you repeat this and move down and to the right through the matrix, you will end up with the identity in place of the original matrix and the inverse to the right! \n",
    "\n",
    "You can assume the following:\n",
    "- the input matrix does not have a zero in the top left corner\n",
    "- the input is of a relatively small size (less than 30x30)\n",
    "- the input is a square matrix. \n",
    "\n",
    "The function input `matrix_dim` is the number of rows of the square input matrix.\n",
    "\n",
    "**Note:** there are a number of ways to actually implement matrix inverse. Start with whatever is simplest, then feel free to optimize!\n",
    "\n",
    "### `matrix_inverse_kernel` (2 points)\n",
    "This kernel will call the `matrix_inverse_inner` function and should do the following:\n",
    "1. move the device matrix into shared memory, \n",
    "2. make sure the computation occurs in shared memory, and\n",
    "3. return the value to device memory. \n",
    "\n",
    "You will note that we specified dynamic shared memory. Please use that and make sure to allocate enough of it in the host function to support however much your device function needs! As with `matrix_inverse_inner`, the function input `matrix_dim` is the number of rows of the input matrix.\n",
    "\n",
    "###  `matrix_inverse` (1 point)\n",
    "This host function will call the `matrix_inverse_kernel` and should do the following:\n",
    "1. move the input host matrix into device memory, \n",
    "2. launch the kernel (with dynamic shared memory), and \n",
    "3. return the solution to host memory. \n",
    "\n",
    "Again, the function input `matrix_dim` is the number of rows of the input matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "**Autograder**: the Gradescope autograder will return failure since this problem set requires a GPU.  To check your solution, here's a Python implementation for reference.  The result of your CUDA code should exactly match the Python output below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=6, floatmode='fixed')\n",
    "\n",
    "def printer(arr):\n",
    "    for row in arr:\n",
    "        print(\" \".join(f\"{x:.6f}\" for x in row))\n",
    "    print()\n",
    "\n",
    "### FIRST TEST\n",
    "test1 = 2*np.eye(5)\n",
    "printer(test1)\n",
    "printer(np.linalg.inv(test1))\n",
    "\n",
    "### SECOND TEST\n",
    "test2 = 7*np.tri(5)\n",
    "printer(test2)\n",
    "printer(np.linalg.inv(test2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XiEPB2MzSaG3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%gpurun -n cuda_mat_inv.cu\n",
    "#include <stdio.h>\n",
    "\n",
    "// GPU Device Function\n",
    "// - actually solve the matrix inverse!\n",
    "__device__\n",
    "void matrix_inverse_inner(float *s_input, float *s_output, float *s_temp, const int matrix_dim){\n",
    "  // Set up the matrix with identity next to it\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "\n",
    "\n",
    "  // Do Guassian elimination walking down the matrix (assuming no leading 0s).\n",
    "  // We therefore use the columns in order as the pivot column for each pivot we need to rescale\n",
    "  // that row so that the pivot value is 1 THEN for all other row values we need to add a multiple\n",
    "  // of the NEW pivot row value such that we transorm the other row pivot column value to 0.\n",
    "  // See https://www.mathsisfun.com/algebra/matrix-inverse-row-operations-gauss-jordan.html\n",
    "  //\n",
    "  // Note if you would prefer to use another method that is fine but/and this is the method\n",
    "  // we have a solution for and are prepared to help you with!\n",
    "\n",
    "  for (unsigned pivRC = 0; pivRC < matrix_dim; pivRC++){\n",
    "      ...\n",
    "      ...\n",
    "      ...\n",
    "      ...\n",
    "      ...\n",
    "      ...\n",
    "      ...\n",
    "      ...\n",
    "      ...\n",
    "  }\n",
    "\n",
    "  // Make sure to write the result to the output\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "}\n",
    "\n",
    "\n",
    "// GPU kernel\n",
    "// - Set up shared memory, run the _inner, clean up shared memory\n",
    "__global__\n",
    "void matrix_inverse_kernel(float *d_input, float *d_output, const int matrix_dim){\n",
    "  // get shared pointers\n",
    "  extern __shared__ float s_dynShared[];\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "\n",
    "  // copy the d_input data into shared memory\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "\n",
    "  // run the code\n",
    "  ...\n",
    "  ...\n",
    "\n",
    "  // copy the memory back out to d_output\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "}\n",
    "\n",
    "\n",
    "// Host function\n",
    "// - set up GPU memory, run the kernel, clean up GPU memory\n",
    "__host__\n",
    "void matrix_inverse(float *h_input, float *h_output, const int matrix_dim){\n",
    "  // transfer memory to the device\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "\n",
    "  // run the kernel\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "\n",
    "  // transfer data back to the host and clean up\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "  ...\n",
    "}\n",
    "\n",
    "\n",
    "// ************************************************\n",
    "//  Main and Utility Functions\n",
    "// ------------------------------------------------\n",
    "//  - You do not need to modify these.\n",
    "//  - They should match the Python\n",
    "//    reference above if your code works!\n",
    "// ************************************************\n",
    "\n",
    "__host__\n",
    "void printMat(float *mat, const int matrix_dim){\n",
    "    // loop through row by row and print\n",
    "    for (int r = 0; r < matrix_dim; r++){\n",
    "        for (int c = 0; c < matrix_dim; c++){\n",
    "            printf(\"%f \",mat[r + c*matrix_dim]);\n",
    "        }\n",
    "        // Newline for new row\n",
    "        printf(\"\\n\");\n",
    "    }\n",
    "    // Newline for end of print\n",
    "    printf(\"\\n\");\n",
    "}\n",
    "\n",
    "__host__\n",
    "void runTest(float *h_input, float *h_output, const int matrix_dim){\n",
    "  // print the input matrix\n",
    "  printMat(h_input,matrix_dim);\n",
    "\n",
    "  // run the main function\n",
    "  matrix_inverse(h_input,h_output,matrix_dim);\n",
    "\n",
    "  // print the final result\n",
    "  printMat(h_output,matrix_dim);\n",
    "}\n",
    "\n",
    "__host__\n",
    "int main() {\n",
    "\n",
    "  // initialize the first test matrix\n",
    "  const int matrix_dim = 5;\n",
    "  const int matrix_dim_sq = matrix_dim*matrix_dim;\n",
    "  float *h_input = (float *)malloc(matrix_dim_sq*sizeof(float));\n",
    "  float *h_output = (float *)malloc(matrix_dim_sq*sizeof(float));\n",
    "  for (int c = 0; c < matrix_dim; c++){\n",
    "      for (int r = 0; r < matrix_dim; r++){\n",
    "          h_input[r + c*matrix_dim] = (r == c) ? 2 : 0;\n",
    "      }\n",
    "  }\n",
    "  // run the test\n",
    "  runTest(h_input,h_output,matrix_dim);\n",
    "\n",
    "  // now do the second test\n",
    "  for (int c = 0; c < matrix_dim; c++){\n",
    "      for (int r = 0; r < matrix_dim; r++){\n",
    "          h_input[r + c*matrix_dim] = r >= c ? 7 : 0;\n",
    "      }\n",
    "  }\n",
    "  runTest(h_input,h_output,matrix_dim);\n",
    "\n",
    "  return 0;\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "q1-matrix-inv": {
     "name": "q1-matrix-inv",
     "points": 4,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
